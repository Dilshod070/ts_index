{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-  \n",
    "# Опыты так сказать с:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import doc2words\n",
    "import docreader\n",
    "from struct import pack, unpack, calcsize\n",
    "from hashlib import md5, algorithms\n",
    "\n",
    "from compress import Varbyte, Simple9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./dataset/lenta.ru_4deb864d-3c46-45e6-85f4-a7ff7544a3fb_01.gz']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = './dataset'\n",
    "FILES = os.listdir(DATA_PATH)\n",
    "FILES = [DATA_PATH + '/' + _file for _file in sorted(FILES[:1])]\n",
    "FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = docreader.DocumentStreamReader(FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 ms, sys: 5.39 ms, total: 32.3 ms\n",
      "Wall time: 31.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_dict = {}\n",
    "for docID, doc in enumerate(reader):\n",
    "    url = doc.url # не буду делать DocID <=> doc.url\n",
    "    body = doc.body # Там нет ни в одном документе тела\n",
    "    text = doc.text\n",
    "    words = doc2words.extract_words(text)\n",
    "    for word in words:\n",
    "        word = unicode(word).encode('utf8')\n",
    "        termID = int(md5(word).hexdigest(), 16) # get int value of hash\n",
    "        if termID in _dict:\n",
    "            if docID not in _dict[termID]:\n",
    "                _dict[termID] = np.concatenate((_dict[termID], [docID]))\n",
    "        else:\n",
    "            _dict[termID] = np.asarray([docID])\n",
    "    if docID == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311878378528305137774189393495128081847: [6]\n",
      "220798841255043928211217796664636418996: [10]\n",
      "307450413009592169440822615273533403989: [4]\n",
      "132472370778067497765503863393117737714: [ 0  1  2  3  7 10]\n",
      "206652403000100525148608061927411431115: [1]\n",
      "299029326086221060415830536176903133998: [1]\n",
      "7336478196583621385623378817424033646: [ 0  2  3  5 10]\n",
      "63548883548822514986286344258916628542: [1]\n",
      "69986279194967900595372218141405592701: [2]\n",
      "22935343108867884981181042812720467261: [3]\n",
      "27345436137887647652709782145180837294: [7]\n",
      "284420083180970213176816728746115774957: [3]\n",
      "222372557009841293096181427623663036811: [10]\n",
      "117148122059749065314445825367308118613: [10]\n",
      "77242452832178521887592784196652095852: [6]\n",
      "291439818284753552045801114852863196068: [ 8 10]\n",
      "35999664102191865165066689740873200004: [3]\n",
      "302733654877821793922912767178351320795: [1]\n",
      "54856839013676489318636616771634651576: [7]\n",
      "190885061693855529773532119764352883513: [7]\n",
      "42306242971694659741827129090935981113: [7]\n"
     ]
    }
   ],
   "source": [
    "for i, _hash in enumerate(_dict):\n",
    "    print str(_hash) + ': ' + str(_dict[_hash])\n",
    "    if i == 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer():\n",
    "    \"\"\"\n",
    "    Makes, Compresses, and Saves dict\n",
    "    \"\"\"\n",
    "    def __init__(self, compressor='varbyte', hash_='md5', coding='utf8'):\n",
    "        \"\"\"\n",
    "        compressors: ['varbyte', 'simple9']\n",
    "        hash_: ['md5'] (hash_ must have hexdigest() attribute)\n",
    "        coding: any (tested only with utf8)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dict = {}\n",
    "        self.compressed_dict = None\n",
    "\n",
    "        if compressor == 'varbyte': self.compressor = Varbyte()\n",
    "        elif compressor == 'simple9': self.compressor = Simple9()\n",
    "        else: raise RuntimeError('Compressor: {} not found'.format(compressor))\n",
    "            \n",
    "        if hash_ == 'md5': self.hash = md5\n",
    "        else: raise RuntimeError('Hash: {} not found'.format(hash_))\n",
    "            \n",
    "        self.coding = coding\n",
    "     \n",
    "    def make_dict(self, doc_reader):\n",
    "        for docID, doc in enumerate(doc_reader):\n",
    "            if docID % 128 != 0: continue\n",
    "            url = doc.url # не буду делать DocID <=> doc.url\n",
    "            body = doc.body # Там нет ни в одном документе тела\n",
    "            text = doc.text\n",
    "            words = doc2words.extract_words(text)\n",
    "            for word in words:\n",
    "                word = unicode(word).encode(self.coding)\n",
    "                termID = int(self.hash(word).hexdigest(), 16) # get int value of hash\n",
    "                if termID in self.dict:\n",
    "                    if docID not in self.dict[termID]:\n",
    "                        self.dict[termID] = np.concatenate((self.dict[termID], [docID]))\n",
    "                else:\n",
    "                    self.dict[termID] = np.asarray([docID])\n",
    "        return\n",
    "    \n",
    "    def compress(self):\n",
    "        if self.compressed_dict is not None:\n",
    "            print >> sys.stderr, 'Dictionary was already compressed'\n",
    "        self.compressed_dict = {}\n",
    "        for termID in self.dict:\n",
    "            self.compressed_dict[termID] = self.compressor.pack(self.dict[termID])\n",
    "        # Удалить словарь, чтобы не занимал память?\n",
    "        self.dict = {}\n",
    "        \n",
    "    def save(self, OUTPUT_FILE='index.dat'):\n",
    "        if self.compressed_dict is None:\n",
    "            raise RuntimeError('Compress dictionaty before saving')\n",
    "        output = open(OUTPUT_FILE, 'wb')\n",
    "        for termID in self.compressed_dict:\n",
    "            array = self.compressed_dict[termID]\n",
    "            # 128bits for termID, 64bist for array_size and array\n",
    "            output.write(pack('QQQ%sB' % array.shape, termID / 2**64, termID % 2**64, array.shape[0], *array))\n",
    "        output.close()\n",
    "        \n",
    "    def _save(self, OUTPUT_FILE='index.dat'):\n",
    "        if self.compressed_dict is None:\n",
    "            raise RuntimeError('Compress dictionaty before saving')\n",
    "        output = open(OUTPUT_FILE, 'wb')\n",
    "        for termID in self.compressed_dict:\n",
    "            array = self.compressed_dict[termID]\n",
    "            output.write(pack('QQ%sB' % array.shape, termID / 2**64, termID % 2**64,  *array))\n",
    "        output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 86 ms, sys: 4.45 ms, total: 90.4 ms\n",
      "Wall time: 89.4 ms\n"
     ]
    }
   ],
   "source": [
    "Idxr = Indexer()\n",
    "%time Idxr.make_dict(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idxr.compress()\n",
    "Idxr.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
